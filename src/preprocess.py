 # preprocess.py
import os
import json
import glob
import pandas as pd
from datetime import datetime, date

from src.common.logging_setup import setup_logging
from src.common.paths import RAW_DIR, PROCESSED_DIR

logger = setup_logging(__name__)


# --- 1. ë„ìš°ë¯¸ í•¨ìˆ˜ë“¤ ì •ì˜ ---
def _create_dataframe_from_list(data_list: list) -> pd.DataFrame:
    """ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    df_list = []
    for item in data_list:
        filepath = item["filepath"]
        raw_data = item["raw_data"]

        flights = raw_data.get("data", {}).get("data", [])
        if flights:
            temp = pd.DataFrame(flights)
            # íŒŒì¼ ê²½ë¡œ(filepath)ì—ì„œ í•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            parts = filepath.split(os.sep)
            temp["source_file"] = filepath
            temp["agency_code"] = os.path.splitext(os.path.basename(filepath))[0].split("_")[-1]
            try:
                temp["scraped_date"] = parts[parts.index("raw") + 1]
            except (ValueError, IndexError):
                temp["scraped_date"] = None
            df_list.append(temp)

    if not df_list:
        return pd.DataFrame()
    return pd.concat(df_list, ignore_index=True)

def _load_data_from_files(file_paths: list) -> pd.DataFrame:
    df_list = []
    for file_path in file_paths:
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                raw = json.load(f)
                flights = raw.get("data", {}).get("data", [])
                if flights:
                    temp = pd.DataFrame(flights)
                    parts = file_path.split(os.sep)
                    temp["source_file"] = file_path
                    temp["agency_code"] = os.path.splitext(os.path.basename(file_path))[0].split("_")[-1]
                    temp["scraped_date"] = parts[parts.index("raw") + 1]
                    df_list.append(temp)
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {file_path} ({e})")

    if not df_list:
        logger.warning("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    return pd.concat(df_list, ignore_index=True)

def _clean_and_transform_data(df: pd.DataFrame) -> pd.DataFrame:
    desc_map = {
        "ë¶€ì‚°/ê¹€í•´": "ë¶€ì‚°", "ë¶€ì‚°(ê¹€í•´)": "ë¶€ì‚°", "ì„œìš¸/ê¹€í¬": "ê¹€í¬", "ì„œìš¸(ê¹€í¬)": "ê¹€í¬",
        "ì§„ì£¼/ì‚¬ì²œ": "ì‚¬ì²œ", "ì§„ì£¼(ì‚¬ì²œ)": "ì‚¬ì²œ", "ì§„ì£¼": "ì‚¬ì²œ", "HIN": "ì‚¬ì²œ",
        "í¬í•­ê²½ì£¼": "í¬í•­", "í¬í•­/ê²½ì£¼": "í¬í•­", "KPO": "í¬í•­",
        "ì—¬ìˆ˜/ìˆœì²œ": "ì—¬ìˆ˜", "RSU": "ì—¬ìˆ˜", "GMP": "ê¹€í¬", "PUS": "ë¶€ì‚°", "CJU": "ì œì£¼",
        "KWJ": "ê´‘ì£¼", "CJJ": "ì²­ì£¼", "TAE": "ëŒ€êµ¬", "USN": "ìš¸ì‚°", "KUV": "êµ°ì‚°",
        "WJU": "ì›ì£¼", "YNY": "ì–‘ì–‘"
    }
    df["depDesc"] = df["depDesc"].replace(desc_map)
    df["arrDesc"] = df["arrDesc"].replace(desc_map)
    depDesc_empty = df["depDesc"].str.strip() == ""
    arrDesc_empty = df["arrDesc"].str.strip() == ""
    df.loc[depDesc_empty, "depDesc"] = df.loc[depDesc_empty, "depCity"].map(desc_map)
    df.loc[arrDesc_empty, "arrDesc"] = df.loc[arrDesc_empty, "arrCity"].map(desc_map)

    for col in ["depDate", "arrDate"]:
        date_str = df[col].astype(str).str.strip().str.replace(r"\.0$", "", regex=True)
        date_str = date_str.where(date_str.str.match(r"^\d{8}$"), pd.NA)
        df[col] = pd.to_datetime(date_str, format="%Y%m%d", errors="coerce")

    df["depDay"] = df["depDate"].dt.day_name().str[:3].str.upper()
    df["arrDay"] = df["arrDate"].dt.day_name().str[:3].str.upper()

    for col in ["depTime", "arrTime"]:
        time_str = df[col].astype(str).str.zfill(4)
        df[col] = pd.to_datetime(time_str, format="%H%M", errors="coerce").dt.time

    df["carDesc"] = df["carDesc"].replace({"ì•„ì‹œì•„ë‚˜í•­ê³µ": "ì•„ì‹œì•„ë‚˜"})
    airline_map = {
        "OZ": "ì•„ì‹œì•„ë‚˜", "KE": "ëŒ€í•œí•­ê³µ", "BX": "ì—ì–´ë¶€ì‚°", "LJ": "ì§„ì—ì–´",
        "TW": "í‹°ì›¨ì´í•­ê³µ", "7C": "ì œì£¼í•­ê³µ", "ZE": "ì´ìŠ¤íƒ€í•­ê³µ", "RS": "ì—ì–´ì„œìš¸", "WE": "íŒŒë¼íƒ€í•­ê³µ"
    }
    df["carDesc_official"] = df["carCode"].map(airline_map)

    for col in ["carDesc", "opCarDesc", "opCarCode"]:
        if col in df.columns:
            df[col] = df[col].astype(object)
            df[col] = df[col].where(df[col].notna(), "")
            df[col] = df[col].astype(str).str.strip()
            df[col] = df[col].replace({"nan": "", "None": "", "NaT": ""})

    mask_we = (df["carCode"] == "WE")

    mask_we_fix_from_op = mask_we & (df["carDesc"] == "") & (df["opCarDesc"] == "íŒŒë¼íƒ€í•­ê³µ")
    df.loc[mask_we_fix_from_op, "carDesc"] = "íŒŒë¼íƒ€í•­ê³µ"

    mask_we_car_empty = mask_we & (df["carDesc"] == "")
    df.loc[mask_we_car_empty, "carDesc"] = df.loc[mask_we_car_empty, "carDesc_official"]
    df.loc[mask_we, ["opCarCode", "opCarDesc"]] = ""

    mismatch = (df["carDesc"] != df["carDesc_official"]) & (~mask_we)
    df.loc[mismatch, "opCarCode"] = df.loc[mismatch, "carCode"].map({"OZ": "BX", "KE": "LJ"})
    df.loc[mismatch, "opCarDesc"] = df.loc[mismatch, "carDesc"]
    df.loc[mismatch, "carDesc"] = df.loc[mismatch, "carDesc_official"]

    opCarDesc_map = {"BX": "ì—ì–´ë¶€ì‚°", "LJ": "ì§„ì—ì–´"}
    mask_op = (df["opCarCode"].isin(opCarDesc_map.keys())) & (df["opCarDesc"].str.strip() == "")
    df.loc[mask_op, "opCarDesc"] = df["opCarCode"].map(opCarDesc_map)

    for col in ["seat", "fare", "fareOrigin", "airTax", "fuelChg", "tasf"]:
        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

    for col in ["classDesc", "mainFlt"]:
        df[col] = df[col].astype(str).str.strip()

    df["scraped_date"] = pd.to_datetime(df["scraped_date"], format="%Y-%m-%d", errors="coerce")

    df["total_price"] = df["fare"] + df["fuelChg"] + df["airTax"] + df["tasf"]

    return df

def _format_final_df(df: pd.DataFrame) -> pd.DataFrame:
    """ìµœì¢… ì»¬ëŸ¼ ìˆœì„œë¥¼ ì •ë¦¬í•˜ê³  ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ì„ ì œê±°í•©ë‹ˆë‹¤."""
    # `upload.py`ì™€ ë™ê¸°í™”ëœ ìµœì¢… ì»¬ëŸ¼ ëª©ë¡
    final_columns = [
        "agency_code", "code", "depDate", "depDay", "depTime", "depCity", "depDesc",
        "arrDate", "arrDay", "arrTime", "arrCity", "arrDesc", "carCode", "carDesc",
        "opCarCode", "opCarDesc", "mainFlt", "classCode", "classDesc", "seat",
        "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf",
        "scraped_date", "source_file"
    ]

    # í˜¹ì‹œ ëª¨ë¥¼ ëˆ„ë½ ì»¬ëŸ¼ ì¶”ê°€ (ê²°ì¸¡ê°’ìœ¼ë¡œ ì±„ì›Œì§)
    for col in final_columns:
        if col not in df.columns:
            df[col] = pd.NA

    return df[final_columns]


# --- 2. ë©”ì¸ í•¨ìˆ˜ (ê°ë… ì—­í• ) ---
def run_preprocess(collected_data: list = None, save_csv=True):
    logger.info("ì „ì²˜ë¦¬ ì‹œì‘")

    if collected_data:
        # 1. íŒŒì´í”„ë¼ì¸ ëª¨ë“œ (ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„° ì²˜ë¦¬)
        logger.info(f"ë©”ëª¨ë¦¬ë¡œë¶€í„° {len(collected_data)}ê°œ ì‘ë‹µ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        df_raw = _create_dataframe_from_list(collected_data)
    else:
        # 2. ë…ë¦½ ì‹¤í–‰ ëª¨ë“œ (íŒŒì¼ ì‹œìŠ¤í…œ)
        logger.info("íŒŒì¼ ì‹œìŠ¤í…œìœ¼ë¡œë¶€í„° ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        json_files = glob.glob(os.path.join(RAW_DIR, "**", "*.json"), recursive=True)
        if not json_files:
            logger.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        df_raw = _load_data_from_files(json_files)

    if df_raw.empty:
        logger.warning("ì²˜ë¦¬í•  í•­ê³µí¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df_raw

    df_clean = _clean_and_transform_data(df_raw)
    df_final = _format_final_df(df_clean)

    if save_csv:
        output_path = PROCESSED_DIR / "preprocessing_data.csv"
        logger.info(f"CSV ì €ì¥/ëˆ„ì  ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤: {output_path}")

        unique_keys = [
            "agency_code", "code",
            "depDate", "depTime", "depCity",
            "arrDate", "arrTime", "arrCity",
            "carCode", "opCarCode",
            "mainFlt", "classCode", "classDesc"
        ]

        def _normalize_for_dedup(df_in: pd.DataFrame) -> pd.DataFrame:
            dfn = df_in.copy()

            # 1) ë‚ ì§œ â†’ YYYY-MM-DD
            for dcol in ["depDate", "arrDate", "scraped_date"]:
                if dcol in dfn.columns:
                    dfn[dcol] = pd.to_datetime(dfn[dcol], errors="coerce").dt.strftime("%Y-%m-%d")

            # 2) ì‹œê°„ â†’ HH:MM:SS (ëª¨ë“  ì…ë ¥ í¬ë§· í—ˆìš©, zero-padding ê°•ì œ)
            def _to_hms(s):
                s = s.astype(str).str.strip()
                # ë¨¼ì € HH:MM:SS í˜•íƒœ ì‹œë„
                t1 = pd.to_datetime(s, format="%H:%M:%S", errors="coerce")
                # HH:MM í˜•íƒœ ì‹œë„
                mask_na = t1.isna()
                if mask_na.any():
                    t2 = pd.to_datetime(s[mask_na], format="%H:%M", errors="coerce")
                    t1 = t1.where(~mask_na, t2)
                # HHMM í˜•íƒœ ì‹œë„
                mask_na = t1.isna()
                if mask_na.any():
                    t3 = pd.to_datetime(s[mask_na].str.zfill(4), format="%H%M", errors="coerce")
                    t1 = t1.where(~mask_na, t3)
                return t1.dt.strftime("%H:%M:%S")

            for tcol in ["depTime", "arrTime"]:
                if tcol in dfn.columns:
                    dfn[tcol] = _to_hms(dfn[tcol])

            # 3) ë¬¸ìì—´ í‘œì¤€í™”: strip, NaNë¥˜ â†’ ""
            str_cols = [
                "agency_code", "code", "depDay", "depCity", "depDesc",
                "arrDay", "arrCity", "arrDesc",
                "carCode", "carDesc", "opCarCode", "opCarDesc",
                "mainFlt", "classCode", "classDesc", "source_file"
            ]
            for col in str_cols:
                if col in dfn.columns:
                    dfn[col] = dfn[col].fillna("").astype(str).str.strip()
                    dfn[col] = dfn[col].replace({"nan": "", "None": "", "NaT": ""})

            for col in str_cols:
                if col in dfn.columns:
                    dfn[col] = dfn[col].fillna("").astype(str).str.strip()
                    dfn[col] = dfn[col].replace({"nan": "", "None": "", "NaT": ""})

            # 4) ì½”ë“œë¥˜ ëŒ€ë¬¸ì í†µì¼
            for col in ["agency_code", "carCode", "opCarCode", "classCode", "mainFlt", "code"]:
                if col in dfn.columns:
                    dfn[col] = dfn[col].str.upper()

            # 5) ì¤‘ë³µí‚¤ ëˆ„ë½ ì»¬ëŸ¼ ë³´ê°•
            for col in unique_keys:
                if col not in dfn.columns:
                    dfn[col] = ""

            return dfn

        try:
            # ì‹ ê·œ ë°ì´í„° ì •ê·œí™”
            new_norm = _normalize_for_dedup(df_final)

            # ì§„ë‹¨: ì‹ ê·œ ë°ì´í„° ë‚´ë¶€ ì¤‘ë³µ
            if len(new_norm) != len(new_norm.drop_duplicates(subset=unique_keys)):
                logger.warning("ì‹ ê·œ ë°ì´í„° ë‚´ë¶€ì—ì„œ ì¤‘ë³µ í‚¤ ë°œê²¬(ì •ê·œí™” ê¸°ì¤€). ì¤‘ë³µì€ CSV ë³‘í•© ì‹œ ì œê±°ë©ë‹ˆë‹¤.")

            if os.path.exists(output_path):
                logger.info("ê¸°ì¡´ CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")
                # dtype=strë¡œ ì½ì–´ë“¤ì—¬ í¬ë§·ì„ì„ ë°©ì§€, ì´í›„ ì •ê·œí™”
                old_df = pd.read_csv(output_path, dtype=str, encoding="utf-8-sig")
                old_norm = _normalize_for_dedup(old_df)

                before = len(old_norm) + len(new_norm)
                combined_df = pd.concat([old_norm, new_norm], ignore_index=True)

                combined_df.drop_duplicates(subset=unique_keys, keep="last", inplace=True)
                after = len(combined_df)
                removed = before - after

                logger.info(f"ì¤‘ë³µ ì œê±° ê²°ê³¼: {removed}ê±´ ì œê±°, ìµœì¢… {after}ê±´")

                combined_df.to_csv(output_path, index=False, encoding="utf-8-sig")
                logger.info(f"ğŸ’¾ {len(new_norm)}ê±´ ì‹ ê·œ ë³‘í•© ì™„ë£Œ. ì´ {len(combined_df)}ê±´ ì €ì¥.")
            else:
                logger.info("ê¸°ì¡´ CSV íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                new_norm.to_csv(output_path, index=False, encoding="utf-8-sig")
                logger.info(f"ğŸ’¾ {len(new_norm)}ê±´ ì €ì¥ ì™„ë£Œ.")

        except Exception as e:
            logger.error(f"âŒ CSV ì €ì¥/ëˆ„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


    logger.info("ì „ì²˜ë¦¬ ì™„ë£Œ")
    return df_final

if __name__ == "__main__":
    run_preprocess(save_csv=True)