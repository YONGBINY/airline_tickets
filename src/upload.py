# upload_tmp.py
import logging
from typing import Dict, Iterable, List

import pandas as pd
from sqlalchemy import create_engine
from psycopg2.extras import execute_values

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "airline_tickets",
    "user": "postgres",
    "password": "qkrdydqls12!"
}

# í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œ (CSV/DF ëª¨ë‘ ì´ ìˆœì„œë¡œ ë§ì¶° ì‚½ì…)
TARGET_COLUMNS: List[str] = [
    "agency_code", "code",
    "depDate", "depDay", "depTime", "depCity", "depDesc",
    "arrDate", "arrDay", "arrTime", "arrCity", "arrDesc",
    "carCode", "carDesc", "opCarCode", "opCarDesc",
    "mainFlt", "classCode", "classDesc", "seat",
    "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf",
    "scraped_date", "source_file"
]

DROP_COLUMNS = ["fareRecKey", "jejucomId", "itinInfo", "itinInfo2"]


def get_engine(db_config: Dict):
    url = (
        f"postgresql+psycopg2://{db_config['user']}:{db_config['password']}"
        f"@{db_config['host']}:{db_config['port']}/{db_config['database']}"
    )
    return create_engine(url, pool_pre_ping=True)


def prepare_df_for_upload(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ
    - ë‚ ì§œ/ì‹œê°„/ì •ìˆ˜ ì»¬ëŸ¼ íƒ€ì… ì •ë¦¬
    - ì»¬ëŸ¼ ìˆœì„œ TARGET_COLUMNSë¡œ ì •ë ¬
    """
    df = df.copy()

    # 1) ë“œë¡­
    existing_drop = [c for c in DROP_COLUMNS if c in df.columns]
    if existing_drop:
        df = df.drop(columns=existing_drop)
        logger.info(f"âœ… ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°: {existing_drop}")

    # 2) íƒ€ì… ì •ë¦¬ (preprocessì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆì–´ë„ ì•ˆì „ë§ìœ¼ë¡œ í•œ ë²ˆ ë”)
    # ë‚ ì§œ
    for dcol in ["depDate", "arrDate", "scraped_date"]:
        if dcol in df.columns:
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce").dt.date

    # ì‹œê°„
    for tcol in ["depTime", "arrTime"]:
        if tcol in df.columns:
            # ì´ë¯¸ datetime.time ì´ë©´ ê·¸ëŒ€ë¡œ, ë¬¸ìì—´ì´ë©´ ë³€í™˜
            df[tcol] = pd.to_datetime(df[tcol].astype(str), errors="coerce", format="%H:%M:%S").dt.time

    # ì •ìˆ˜
    for icol in ["seat", "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf"]:
        if icol in df.columns:
            df[icol] = pd.to_numeric(df[icol], errors="coerce").fillna(0).astype(int)

    # ë¬¸ìì—´ íŠ¸ë¦¼
    str_cols = [
        "agency_code", "code", "depDay", "depCity", "depDesc",
        "arrDay", "arrCity", "arrDesc",
        "carCode", "carDesc", "opCarCode", "opCarDesc",
        "mainFlt", "classCode", "classDesc", "source_file"
    ]
    for scol in str_cols:
        if scol in df.columns:
            df[scol] = df[scol].astype(str).str.strip()

    # 3) ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ
    missing = [c for c in TARGET_COLUMNS if c not in df.columns]
    if missing:
        raise ValueError(f"ì—…ë¡œë“œì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    df = df[TARGET_COLUMNS]
    logger.info(f"âœ… ì—…ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ: {len(df)}í–‰, ì»¬ëŸ¼ {len(df.columns)}ê°œ")
    return df


def to_tuples(df: pd.DataFrame) -> Iterable[tuple]:
    # í–‰ â†’ íŠœí”Œ ë³€í™˜ (NaN â†’ None)
    return (tuple(None if pd.isna(x) else x for x in row) for row in df.to_numpy())


def upload_to_db(df: pd.DataFrame, engine, batch_size: int = 5000):
    """
    ON CONFLICTë¡œ ì•ˆì „ ì‚½ì…, ë°°ì¹˜ ì²˜ë¦¬, ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
    """
    if df.empty:
        logger.warning("âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    insert_sql = """
    INSERT INTO "flight_info" (
        "agency_code", "code", "depDate", "depDay", "depTime", "depCity", "depDesc",
        "arrDate", "arrDay", "arrTime", "arrCity", "arrDesc",
        "carCode", "carDesc", "opCarCode", "opCarDesc",
        "mainFlt", "classCode", "classDesc", "seat",
        "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf",
        "scraped_date", "source_file"
    ) VALUES %s
    ON CONFLICT ON CONSTRAINT "unique_flight" DO NOTHING;
    """

    total_rows = len(df)
    logger.info(f"ğŸ“¤ ì—…ë¡œë“œ ì‹œì‘: {total_rows}ê±´ (ë°°ì¹˜ {batch_size})")

    for start in range(0, total_rows, batch_size):
        batch = df.iloc[start:start + batch_size]
        tuples = list(to_tuples(batch))

        with engine.begin() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            try:
                execute_values(cursor, insert_sql, tuples, page_size=batch_size)
                raw_conn.commit()
                logger.info(f"âœ… {start + len(batch)}/{total_rows}í–‰ ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                raw_conn.rollback()
                logger.error(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨ @ {start}: {e}")
                raise

    logger.info("ğŸ‰ ì „ì²´ ì—…ë¡œë“œ ì™„ë£Œ")


def run_upload(df: pd.DataFrame, db_config: Dict = DB_CONFIG, batch_size: int = 5000):
    """
    preprocessì—ì„œ ë°˜í™˜ëœ DataFrameì„ ì§ì ‘ ë°›ì•„ ì—…ë¡œë“œ
    """
    if df is None or df.empty:
        logger.warning("âš ï¸ ì…ë ¥ DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ì¤‘ë‹¨.")
        return

    # ì—…ë¡œë“œ ì „ ì •ë¦¬
    df_prepared = prepare_df_for_upload(df)

    # ì—”ì§„ ìƒì„±
    engine = get_engine(db_config)
    logger.info("âœ… DB ì—°ê²° ì„±ê³µ")

    # ì—…ë¡œë“œ
    upload_to_db(df_prepared, engine, batch_size=batch_size)


# ìœ ì—°ì„±ì„ ìœ„í•´ CSVë¡œë¶€í„° ì§ì ‘ ì—…ë¡œë“œí•˜ëŠ” ìœ í‹¸ (ì˜µì…˜)
def run_upload_from_csv(csv_path: str, db_config: Dict = DB_CONFIG, batch_size: int = 5000):
    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    logger.info(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰ from {csv_path}")
    run_upload(df, db_config=db_config, batch_size=batch_size)


if __name__ == "__main__":
    # í•„ìš”ì‹œ ìˆ˜ë™ í…ŒìŠ¤íŠ¸ìš© (CSV ê²½ë¡œë¥¼ ë„£ì–´ ì‚¬ìš©)
    run_upload_from_csv("../data/processed/preprocessing data.csv")
    pass