# upload.py
import os
from typing import Dict, Iterable, List
import pandas as pd
from sqlalchemy import create_engine
from psycopg2.extras import execute_values


from src.common.logging_setup import setup_logging
from src.common.paths import PROCESSED_DIR
from src.common.config import DB_CONFIG

logger = setup_logging(__name__)

TARGET_COLUMNS: List[str] = [
    "agency_code", "code",
    "depDate", "depDay", "depTime", "depCity", "depDesc",
    "arrDate", "arrDay", "arrTime", "arrCity", "arrDesc",
    "carCode", "carDesc", "opCarCode", "opCarDesc",
    "mainFlt", "classCode", "classDesc", "seat",
    "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf",
    "scraped_date", "source_file"
]

DROP_COLUMNS = ["fareRecKey", "jejucomId", "itinInfo", "itinInfo2"]


def get_engine(db_config: Dict):
    url = (
        f"postgresql+psycopg2://{db_config['user']}:{db_config['password']}"
        f"@{db_config['host']}:{db_config['port']}/{db_config['database']}"
    )
    return create_engine(url, pool_pre_ping=True)

def prepare_df_for_upload(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ
    - ë‚ ì§œ/ì‹œê°„/ì •ìˆ˜ ì»¬ëŸ¼ íƒ€ì… ì •ë¦¬
    - ì»¬ëŸ¼ ìˆœì„œ TARGET_COLUMNSë¡œ ì •ë ¬
    """
    df = df.copy()

    # 1) ë“œë¡­
    existing_drop = [c for c in DROP_COLUMNS if c in df.columns]
    if existing_drop:
        df = df.drop(columns=existing_drop)
        logger.info(f"âœ… ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°: {existing_drop}")

    # 2) íƒ€ì… ì •ë¦¬ (preprocessì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆì–´ë„ ì•ˆì „ë§ìœ¼ë¡œ í•œ ë²ˆ ë”)
    # ë‚ ì§œ
    for dcol in ["depDate", "arrDate", "scraped_date"]:
        if dcol in df.columns:
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce").dt.date

    # ì‹œê°„
    for tcol in ["depTime", "arrTime"]:
        if tcol in df.columns:
            # ì´ë¯¸ datetime.time ì´ë©´ ê·¸ëŒ€ë¡œ, ë¬¸ìì—´ì´ë©´ ë³€í™˜
            df[tcol] = pd.to_datetime(df[tcol].astype(str), errors="coerce", format="%H:%M:%S").dt.time

    # ì •ìˆ˜
    for icol in ["seat", "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf"]:
        if icol in df.columns:
            df[icol] = pd.to_numeric(df[icol], errors="coerce").fillna(0).astype(int)

    # ë¬¸ìì—´ íŠ¸ë¦¼
    str_cols = [
        "agency_code", "code", "depDay", "depCity", "depDesc",
        "arrDay", "arrCity", "arrDesc",
        "carCode", "carDesc", "opCarCode", "opCarDesc",
        "mainFlt", "classCode", "classDesc", "source_file"
    ]
    for scol in str_cols:
        if scol in df.columns:
            df[scol] = df[scol].astype(str).str.strip()

    # 3) ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ
    missing = [c for c in TARGET_COLUMNS if c not in df.columns]
    if missing:
        raise ValueError(f"ì—…ë¡œë“œì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    df = df[TARGET_COLUMNS]
    logger.info(f"âœ… ì—…ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ: {len(df)}í–‰, ì»¬ëŸ¼ {len(df.columns)}ê°œ")
    return df

def to_tuples(df: pd.DataFrame) -> Iterable[tuple]:
    # í–‰ â†’ íŠœí”Œ ë³€í™˜ (NaN â†’ None)
    return (tuple(None if pd.isna(x) else x for x in row) for row in df.to_numpy())

def upload_to_db(df: pd.DataFrame, engine, batch_size: int = 5000):
    """
    ON CONFLICTë¡œ ì•ˆì „ ì‚½ì…, ë°°ì¹˜ ì²˜ë¦¬, ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
    """
    if df.empty:
        logger.warning("âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    insert_sql = """
    INSERT INTO "flight_info" (
        "agency_code", "code", "depDate", "depDay", "depTime", "depCity", "depDesc",
        "arrDate", "arrDay", "arrTime", "arrCity", "arrDesc",
        "carCode", "carDesc", "opCarCode", "opCarDesc",
        "mainFlt", "classCode", "classDesc", "seat",
        "total_price", "fare", "fareOrigin", "fuelChg", "airTax", "tasf",
        "scraped_date", "source_file"
    ) VALUES %s
    ON CONFLICT ON CONSTRAINT "unique_flight" DO NOTHING;
    """

    total_rows = len(df)
    logger.info(f"ğŸ“¤ ì—…ë¡œë“œ ì‹œì‘: {total_rows}ê±´ (ë°°ì¹˜ {batch_size})")

    for start in range(0, total_rows, batch_size):
        batch = df.iloc[start:start + batch_size]
        tuples = list(to_tuples(batch))

        with engine.begin() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            try:
                execute_values(cursor, insert_sql, tuples, page_size=batch_size)
                raw_conn.commit()
                logger.info(f"âœ… {start + len(batch)}/{total_rows}í–‰ ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                raw_conn.rollback()
                logger.error(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨ @ {start}: {e}", exc_info=True)
                raise

    logger.info("ğŸ‰ ì „ì²´ ì—…ë¡œë“œ ì™„ë£Œ")

def run_upload(df: pd.DataFrame, batch_size: int = 5000):
    logger.info("DB ì—…ë¡œë“œ ì‹œì‘")
    if df is None or df.empty:
        logger.warning("âš ï¸ ì…ë ¥ DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ì¤‘ë‹¨.")
        return

    df_prepared = prepare_df_for_upload(df)
    engine = get_engine(DB_CONFIG)
    logger.info("âœ… DB ì—°ê²° ì„±ê³µ")
    upload_to_db(df_prepared, engine, batch_size=batch_size)
    logger.info("DB ì—…ë¡œë“œ ì™„ë£Œ")

def run_upload_from_csv(csv_path: str, batch_size: int = 5000):
    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    logger.info(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰ from {csv_path}")
    run_upload(df, batch_size=batch_size)


if __name__ == "__main__":
    run_upload_from_csv(PROCESSED_DIR/"preprocessing data.csv")
    pass